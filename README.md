# VLMOD: Understanding Multi-Object World from Monocular View

> æœ¬ä»“åº“ä¸ºã€Œ2025 VLP æŒ‘æˆ˜èµ›å‚èµ›ä½œå“ã€ã€‚

Author: Keyu Guo, Yongle Huang, Shijie Sun, Xiangyu Song, Mingtao Feng, Zedong Liu, Huansheng Song, Tiantian Wang, Jianxin Li, Naveed Akhtar and Ajmal Saeed Mian



The paper has been accepted by **2025 IEEE Conference on Computer Vision and Pattern Recognition (CVPR2025)** ğŸ‰.

<p align="center">

    <img src="https://i.imgur.com/waxVImv.png" alt="Oryx Video-ChatGPT">This repository provides **partial code** for the **VLMOD Challenge (Track B)** â€” *Understanding Multi-Object World from Monocular View*.  

Repository: https://github.com/Lidk1/MemoryWontLeak_VLMOD



![VLMOD.png](img/VLMOD.png)



The task focuses on **multi-object 3D Visual Grounding (3DVG)** based on **a single monocular RGB image**, enabling machines to interpret complex scenes and spatial relationships using natural language.



## ğŸ§  Task Description
Given a monocular RGB image and a complex language description (e.g., *"find the red cup on the left side of the table and the black keyboard on the right side"*),  
the goal is to predict **each referred objectâ€™s**:
- 3D position (x, y, z)
- 3D size (width, height, depth)
- Orientation (rotation angle)

## ğŸš§ Core Challenges
- Multi-object scene parsing  
- Spatial relationship modeling  
- Accurate 3D property estimation  

## ğŸ“‚ Code Release
We have **open-sourced part of our implementation** to help the community explore and reproduce results.  
You are encouraged to:

- Reproduce and verify the released modules  
- Implement or improve other components  
- Contribute new ideas for monocular 3D visual grounding  

## ğŸš€ Quick Start (Baseline Grounding)

This repo includes a simple grounding baseline implemented in `libs.py` that parses the provided JSON annotations (MonoMulti3D-ROPE) and matches objects to a language query using rule-based constraints and an optional KAN-based ranking.

- Example (Python):

```python
from libs import ground_from_json, batch_ground

# Single file grounding
json_path = r"x:\MonoMulti-3DVG-main\MonoMulti3D-ROPE\train\jsons\1632_fa2sd4a11North151_420_1613710840_1613716786_1_obstacle.json"
query = "æ‰¾å‡ºå·¦ä¸Šè§’çš„é»‘è‰²æ±½è½¦"
result = ground_from_json(json_path, query)
print(result)

# Optional: apply KAN-based ranking to matches
from libs import rank_matches_with_kan
result["matches"] = rank_matches_with_kan(result["matches"])

# Batch grounding on a directory (limit for preview)
dir_path = r"x:\MonoMulti-3DVG-main\MonoMulti3D-ROPE\train\jsons"
results = batch_ground(dir_path, query, limit=5)
for r in results:
    print(r["json_path"], len(r["matches"]))
```

- Output format per match:
  - `category`: object type (e.g., `car`)
  - `3d_coords`: `[X, Y, Z]`
  - `3d_dims`: `[width, height, length]`
  - `yaw`: rotation angle (radians)
  - `bbox`: `[x1, y1, x2, y2]`
  - `appearance`: color

Notes:
- Images are not included in this release; the baseline uses bounding box distribution in JSON to approximate spatial layout (e.g., left/right/top/bottom, quadrants).
- The baseline supports bilingual queries (English/Chinese) for colors, places, and simple relations.
- It also supports numeric constraints for dimensions and distance in queries:
  - Dimension ranges: "é•¿åº¦4.1åˆ°4.5ç±³" / "length: 4.1 to 4.5 m"ï¼ˆæ”¯æŒ é•¿/å®½/é«˜ï¼‰
  - Distance: "è·ç¦»çº¦100ç±³" / "distance: 100 m"ï¼ˆæŒ‰Zè½´è¿‡æ»¤ï¼Œå®¹å·®Â±10%æˆ–Â±5mï¼‰
  - ç¤ºä¾‹ï¼š`"æ‰¾å‡ºå·¦ä¾§é»‘è‰²æ±½è½¦ï¼Œé•¿åº¦4åˆ°4.5ç±³ï¼Œè·ç¦»çº¦100ç±³"`
- For full 3DVG training from RGB, integrate an image encoder and detector, then replace the rule-based matcher with multimodal attention and relation modeling.

## âš™ï¸ ç¯å¢ƒé…ç½®ï¼ˆrequirementsï¼‰
- Python 3.10+
- å®‰è£…ä¾èµ–ï¼š
  - åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ï¼‰ï¼š`python -m venv .venv && .\.venv\Scripts\activate`
  - å®‰è£…ï¼š`pip install -r requirements.txt`
- è¯´æ˜ï¼š`torch` éœ€æ ¹æ®æœ¬æœº CUDA/CPU ç¯å¢ƒé€‰æ‹©åˆé€‚çš„å‘è¡Œç‰ˆï¼›å¦‚éœ€ GPU è®­ç»ƒï¼Œè¯·å‚è€ƒ PyTorch å®˜ç½‘é€‰æ‹©å¯¹åº”çš„ç‰ˆæœ¬ä¸å®‰è£…å‘½ä»¤ã€‚

## ğŸ“„ å¼€æºè®¸å¯ä¸ä½¿ç”¨
- è®¸å¯è¯ï¼šæœ¬é¡¹ç›®é‡‡ç”¨ `MIT License`ï¼ˆè§ä»“åº“ `LICENSE` æ–‡ä»¶ï¼‰ã€‚
- ç¦æ­¢ä½¿ç”¨é™åˆ¶æ€§è®¸å¯è¯ï¼ˆå¦‚ GPL ç­‰ï¼‰ï¼Œæœ¬ä»“åº“å·²ç¬¦åˆèµ›äº‹å¼€æºè¦æ±‚ï¼šå…¬å¼€å¯è®¿é—®ã€åŒ…å«ç¯å¢ƒé…ç½®ï¼ˆ`requirements.txt`ï¼‰ã€è¿è¡Œè¯´æ˜ï¼ˆREADMEï¼‰ã€‚

## ğŸ¯ æ›´ç²¾ç¡®çš„æ¨¡å‹ï¼ˆè®­ç»ƒæ‰“åˆ†å™¨ï¼‰

ä¸ºæå‡åŒ¹é…ç²¾åº¦ï¼Œå¯ä»¥ä½¿ç”¨åŸºäºè®­ç»ƒçš„ `TextConstraintScorer`ã€‚è¯¥æ‰“åˆ†å™¨ä»æ ‡æ³¨ JSON å­¦ä¹ â€œæ•°å€¼èŒƒå›´ã€é¢œè‰²ã€ä½ç½®â€ç­‰çº¦æŸçš„æƒé‡ï¼Œä½¿æ’åºæ›´ç¨³å®šã€æ›´è´´åˆæŸ¥è¯¢ã€‚

### è®­ç»ƒç¤ºä¾‹

```python
from libs import train_text_scorer

root = r"x:\MonoMulti-3DVG-main\MonoMulti3D-ROPE\train\jsons"
save = train_text_scorer(root, epochs=3, lr=1e-3, neg_per_pos=4, file_limit=200)
print("model saved to:", save)
```

- æ•°æ®æ¥æºï¼šåŒä¸€å›¾åƒä¸‹ï¼Œä½¿ç”¨æ¯æ¡ `public_description` çš„æ­£æ ·æœ¬ï¼ˆ`label_3` è§£æå¾—åˆ°çš„å¯¹è±¡ï¼‰ä¸åŒå›¾åƒçš„å…¶å®ƒå¯¹è±¡æ„å»ºæ’åä»»åŠ¡ã€‚
- æŸå¤±å‡½æ•°ï¼šåˆé¡µæ’åæŸå¤±ï¼ˆhinge lossï¼‰ï¼Œæ¨é«˜æ­£æ ·æœ¬åˆ†æ•°ã€å‹ä½è´Ÿæ ·æœ¬åˆ†æ•°ã€‚
- å¯é€‰å‚æ•°ï¼š
  - `epochs`ï¼ˆé»˜è®¤ 3ï¼‰ï¼šè®­ç»ƒè½®æ¬¡ã€‚
  - `lr`ï¼ˆé»˜è®¤ 1e-3ï¼‰ï¼šå­¦ä¹ ç‡ã€‚
  - `neg_per_pos`ï¼ˆé»˜è®¤ 4ï¼‰ï¼šæ¯ä¸ªæ­£æ ·æœ¬é…å¯¹çš„è´Ÿæ ·æœ¬æ•°é‡ã€‚
  - `file_limit`ï¼šé™åˆ¶ç”¨äºè®­ç»ƒçš„æ–‡ä»¶æ•°é‡ï¼ˆä¾¿äºå¿«é€Ÿè¯•è·‘ï¼‰ã€‚
  - `save_path`ï¼šæ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤ `./text_scorer.pt`ï¼‰ã€‚

### æ¨ç†ç¤ºä¾‹ï¼ˆç²¾ç¡®æ’åºï¼‰

```python
from libs import ground_from_json_precise

json_path = r"x:\MonoMulti-3DVG-main\MonoMulti3D-ROPE\train\jsons\1632_fa2sd4a11North151_420_1613710840_1613716786_10_obstacle.json"
query = "height : 1.4 to 1.8 m, appearance : white, distance : 59.9 m"
model_path = r"./text_scorer.pt"

res = ground_from_json_precise(json_path, query, model_path)
for m in res["matches"][:3]:  # æŸ¥çœ‹Top-3
    print(m["score"], m["category"], m["3d_coords"], m["3d_dims"], m["appearance"], m["bbox"]) 
```

- è¿”å›ä¸­æ¯é¡¹åŒ…å« `score`ï¼ˆåˆ†æ•°è¶Šå¤§è¶Šç¬¦åˆæŸ¥è¯¢ï¼‰ä»¥åŠåŸºç¡€ç‰ˆç›¸åŒçš„å¯¹è±¡å­—æ®µï¼ˆ`category`, `3d_coords`, `3d_dims`, `yaw`, `bbox`, `appearance`ï¼‰ã€‚
- è‹¥éœ€å¼•å…¥â€œæœå‘/é®æŒ¡â€ç­‰æ›´å¤šçº¦æŸï¼Œå¯åœ¨ `extract_constraints` æ‰©å±•è§£æï¼Œå¹¶åœ¨ `build_constraint_features` è¡¥å……ç‰¹å¾ã€‚

## ğŸ“¦ æäº¤ç”Ÿæˆå™¨ï¼ˆVLMOD Task 2ï¼‰
æœ¬ä»“åº“åŒ…å«ä¸€ä¸ªç”¨äºå®˜æ–¹è¯„æµ‹æäº¤çš„ç”Ÿæˆå™¨ï¼š`generate_submission_txt.py`ã€‚

- åŠŸèƒ½ï¼šä¸º `MonoMulti3D-ROPE/test` ä¸‹çš„æ¯ä¸ª `*_obstacle.json` ç”ŸæˆåŒå `.txt` æ–‡ä»¶ï¼ˆä½äº `submission_txt/`ï¼‰ã€‚
- è¾“å‡ºæ ¼å¼ï¼šæ¯è¡Œä¸‰åˆ—æ•´æ•°ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ï¼Œåˆ†åˆ«å¯¹åº” `public_description` çš„å‰ä¸‰æ¡æè¿°å¯¹è¯¥å¯¹è±¡çš„åŒ¹é…ç»“æœï¼›å½“å‰ç‰ˆæœ¬ä¸ºäºŒå€¼ `{0,1}`ï¼ˆæœªä½¿ç”¨ `2`ï¼‰ã€‚
- æ‰“åŒ…ï¼šç”Ÿæˆåå¯å‹ç¼©ä¸º `submission_txt.zip` ç”¨äºä¸Šä¼ ï¼ˆè§ä¸‹ï¼‰ã€‚

### ä½¿ç”¨
```powershell
# ç”Ÿæˆæ‰€æœ‰æäº¤æ–‡ä»¶ï¼ˆWindows/PowerShellï¼‰
python generate_submission_txt.py

# å¯é€‰ï¼šæ‰“åŒ…ä¸º zip ç”¨äºä¸Šä¼ 
Compress-Archive -Path submission_txt\* -DestinationPath submission_txt.zip
```

### è¯´æ˜
- è¯»å–å­—æ®µï¼š`public_description`ï¼ˆæ–‡æœ¬çº¦æŸï¼‰ã€`test_data`ï¼ˆå€™é€‰å¯¹è±¡è¡Œï¼‰ã€‚
- çº¦æŸè§£æï¼š`libs.extract_constraints` ä¸ `place_filter`ï¼Œæ”¯æŒé¢œè‰²ã€ç±»åˆ«ã€æ–¹ä½ã€å°ºå¯¸/è·ç¦»åŒºé—´ç­‰ã€‚
- æ¨ç†å‡çº§ï¼šè®­ç»ƒå®Œæˆåï¼Œæ¨èä½¿ç”¨ `TextConstraintScorer` çš„æ‰“åˆ†æ›¿æ¢è§„åˆ™åŒ¹é…ï¼Œä»¥æå‡æŒ‡æ ‡ã€‚

æ•°æ®è¯´æ˜ï¼š`MonoMulti3D-ROPE/test` ç›®å½•å·²åŒ…å«è¯„æµ‹ JSONï¼›ç¤ºä¾‹ä¸­çš„è®­ç»ƒè·¯å¾„ï¼ˆå¦‚ `.../train/jsons`ï¼‰ä¸ºå¤–éƒ¨æ•°æ®ï¼Œéœ€è‡ªè¡Œä¸‹è½½å¹¶è°ƒæ•´ä¸ºæœ¬åœ°ç»å¯¹è·¯å¾„ã€‚

## ğŸ¤ Contribution
We welcome open discussions, reproduction efforts, and performance comparisons.  
Please feel free to submit issues or pull requests to share your work.

## ğŸ“œ License
This project is released for **academic and research purposes** only.



## **ğŸ·ï¸ Citation**

```bibtex
@inproceedings{guo2025beyond,
  title={Beyond Human Perception: Understanding Multi-Object World from Monocular View},
  author={Guo, Keyu and Huang, Yongle and Sun, Shijie and Song, Xiangyu and Feng, Mingtao and Liu, Zedong and Song, Huansheng and Wang, Tiantian and Li, Jianxin and Akhtar, Naveed and others},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={3751--3760},
  year={2025}
}
```

